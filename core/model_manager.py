# model_manager.py
import asyncio
import logging
from typing import Dict
from faster_whisper import WhisperModel

from core.model_loader import get_model, loaded_models, _model_load_lock

logger = logging.getLogger(__name__)

# å…¨å±€æ¨¡å‹ç¼“å­˜
_model_cache: Dict[str, WhisperModel] = {}


async def get_cached_model(model_name: str, device: str, compute_type: str) -> WhisperModel:
    """
    è·å–ç¼“å­˜ä¸­çš„æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åŠ è½½å¹¶ç¼“å­˜
    """
    key = f"{model_name}_{device}_{compute_type}"

    # å¿«é€Ÿæ£€æŸ¥ç¼“å­˜
    if key in loaded_models:
        logger.info(f"âœ… ä»ç¼“å­˜ä¸­è·å–æ¨¡å‹: {key}")
        return loaded_models[key]

    logger.info(f"ğŸ”„ æ¨¡å‹ {key} æœªåœ¨ç¼“å­˜ä¸­ï¼Œå¼€å§‹å¼‚æ­¥åŠ è½½...")
    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŸæœ‰çš„åŒæ­¥åŠ è½½é€»è¾‘
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        None,
        get_model,
        model_name,
        device,
        compute_type
    )


def _load_model_sync(model_name: str, device: str, compute_type: str) -> WhisperModel:
    """åŒæ­¥åŠ è½½æ¨¡å‹çš„è¾…åŠ©å‡½æ•°"""
    return get_model(model_name, device, compute_type)


def clear_model_cache():
    """æ¸…ç©ºæ¨¡å‹ç¼“å­˜"""
    with _model_load_lock:
        loaded_models.clear()
    logger.info("âœ… æ¨¡å‹ç¼“å­˜å·²æ¸…ç©º")


def get_cached_model_names() -> list:
    """è·å–å½“å‰ç¼“å­˜çš„æ‰€æœ‰æ¨¡å‹åç§°"""
    return list(loaded_models.keys())